{
  "name": "datapumps",
  "description": "Node.js ETL (Extract, Transform, Load) toolkit for easy data import, export or transfer between systems.",
  "licence": "MIT",
  "author": {
    "name": "Szilard Novaki",
    "email": "novaki@agmen.hu",
    "url": "https://github.com/novaki"
  },
  "homepage": "https://github.com/agmen-hu/node-datapumps",
  "repository": {
    "type": "git",
    "url": "https://github.com/agmen-hu/node-datapumps"
  },
  "bugs": {
    "url": "https://github.com/agmen-hu/node-datapumps/issues"
  },
  "version": "0.5.1",
  "dependencies": {
    "bluebird": "^2.2.2",
    "coffee-script": "^1.7.1",
    "excel4node": "~0.0.9",
    "fast-csv": "~0.5.2",
    "mongodb": "^2.2.24",
    "mysql": "~2.5.1",
    "pg": "^6.1.5",
    "restler": "~3.2.2",
    "xlsx": "~0.7.8"
  },
  "devDependencies": {
    "grunt": "~0.4.1",
    "grunt-cafe-mocha": "~0.1.12",
    "grunt-contrib-coffee": "~0.11.1",
    "grunt-contrib-watch": "~0.6.1",
    "mocha": "~1.21.3",
    "should": "~4.0.4",
    "sinon": "~1.10.3"
  },
  "scripts": {
    "prepublish": "coffee --no-header -o lib -c src",
    "test": "mocha --compilers coffee:coffee-script/register src/spec src/mixin/spec"
  },
  "keywords": [
    "etl",
    "import",
    "export",
    "batch",
    "excel",
    "xlsx",
    "rest"
  ],
  "readme": "# Datapumps: Simple ETL for node.js\n[![Travis CI Badge](https://api.travis-ci.org/agmen-hu/node-datapumps.svg?branch=master)](https://travis-ci.org/agmen-hu/node-datapumps \"Travis CI\")\n\n## Overview\nUse pumps to import, export, transform or transfer data. A data pump will read from its input stream, array or datapumps Buffer and will write to its output buffers. A pump will finish when all data is consumed from its output buffers. Make a group of pumps to handle complex ETL tasks.\n\n## Installation\n```\n$ npm install datapumps --save\n```\n\n## Usage example: export mongodb to excel\n```js\nvar\n  datapumps = require('datapumps'),\n  Pump = datapumps.Pump,\n  MongodbMixin = datapumps.mixin.MongodbMixin,\n  ExcelWriterMixin = datapumps.mixin.ExcelWriterMixin,\n  pump = new Pump();\n\npump\n  .mixin(MongodbMixin('mongodb://localhost/marketing'))\n  .useCollection('Contact')\n  .from(pump.find({ country: \"US\" }))\n\n  .mixin(ExcelWriterMixin())\n  .createWorkbook('/tmp/ContactsInUs.xlsx')\n  .createWorksheet('Contacts')\n  .writeHeaders(['Name', 'Email'])\n\n  .process(function(contact) {\n    return pump.writeRow([ contact.name, contact.email ]);\n  })\n  .logErrorsToConsole()\n  .run()\n    .then(function() {\n      console.log(\"Done writing contacts to file\");\n    });\n```\n\nUsage example with more details:\n * First, we create a pump and setup reading from mongodb\n   ```js\n   var pump = new Pump();\n   pump\n     .mixin(MongodbMixin('mongodb://localhost/marketing'))\n     .useCollection('Contact')\n     .from(pump.find({ country: \"US\" }))\n   ```\n   Mixins extend the functionality of a pump. The [MongodbMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/MongodbMixin.html)\n   adds `.find()` method which executes a query on the collection specified with `.useCollection()`\n   method. The pump will read the query results and controls data flow, i.e. it pauses read when it\n   cannot write excel rows.\n\n * Write data to excel with [ExcelWriterMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/ExcelWriterMixin.html):\n   ```js\n   pump\n     .mixin(ExcelWriterMixin())\n     .createWorkbook('/tmp/ContactsInUs.xlsx')\n     .createWorksheet('Contacts')\n     .writeHeaders(['Name', 'Email'])\n\n     .process(function(contact) {\n       return pump.writeRow([ contact.name, contact.email ]);\n     })\n   ```\n   The excel workbook, worksheet and header rows are created after adding\n   [ExcelWriterMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/ExcelWriterMixin.html) to the pump.\n   Each pump has a `.process()` callback that may transform or filter data. The callback is called for every data item of the\n   buffer and should return a [promise](https://promisesaplus.com/) (we use [bluebird](https://github.com/petkaantonov/bluebird)\n   library) that fulfills when the data is processed. In this example, the default processing callback\n   (which copies data to the output buffer by default) is overridden with writing rows to the excel\n   worksheet.\n\n * Finally, start the pump and write to console when it's done.\n   ```js\n   pump\n     .logErrorsToConsole()\n     .run()\n       .then(function() {\n         console.log(\"Done writing contacts to file\");\n       });\n   ```\n   The `.logErrorsToConsole()` will log any error to the console, surprisingly. The pump will start\n   on calling `.run()`. It returns a [promise](https://promisesaplus.com/) that resolves when the pump finished.\n\n## Pump\nA pump reads data from its input buffer or stream and copies it to the output buffer by default:\n```js\ndatapumps = require('datapumps');\n(pump = new datapumps.Pump())\n  .from(<put a nodejs stream or datapumps buffer here>)\n  .run()\n```\n\nTo access the output buffer, use the `.buffer()` method, which returns a Buffer instance:\n```js\nbuffer = pump.buffer('output');\nbuffer = pump.buffer(); // equivalent with previous as the default buffer\n                        // of the pump is called 'output'\n```\n\nUse the `.buffers()` method when you need to write data into multiple output buffers:\n```js\nticketsPump\n  .buffers({\n    openTickets: ticketsPump.createBuffer(),\n    closedTickets: ticketsPump.createBuffer(),\n  });\n\nreminderMailer = new datapumps.Pump()\nreminderMailer\n  .from(ticketPump.buffer('openTickets'))\n  ...\n```\nNote that the *ticketsPump* pump has two output buffers: *openTickets* and *closedTickets*. The *reminderMailer* pump\nreads data from the *openTickets* buffer of the *tickets* pump.\n\n### Transforming data\nUse the `.process()` method to set the function which processes data:\n```js\nticketsPump\n  .process(function(ticket) {\n    ticket.title = 'URGENT: ' + ticket.title;\n    return this.buffer('openTickets').writeAsync(ticket);\n  });\n```\nThe argument of `.process()` is a function that will be executed after the pump reads a data item.\nThe function is executed in the context of the pump object, i.e. `this` refers to the pump itself. The\nfunction should return a Promise that fulfills when the data is processed (i.e. written into a buffer\nor stored elsewhere).\n\n### Start and end of pumping\nA pump is started by calling the `.start()` method. The `end` event will be emitted when the\ninput stream or buffer ended and all output buffers became empty.\n```js\npump.on('end', function() {\n  console.log('Pumped everything, and all my output buffers are empty. Bye.')\n})\n```\n\n## Pump group\nYou often need multiple pumps to complete an ETL task. Pump groups help starting multiple pump in\none step, and also enables handling the event when every pump ended:\n```js\nsendMails = datapumps.group();\nsendMails.addPump('tickets')\n  ...;\nsendMails.addPump('reminderMailer')\n  ...;\nsendMails\n  .start()\n  .whenFinished().then(function() {\n    console.log('Tickets processed.');\n  });\n```\nThe `.addPump()` method creates a new pump with given name and returns it for configuration.\n`.start()` will start all pumps in the group, while `.whenFinished()` returns a Promise the fulfills\nwhen every pump ended (Note: `end` event is also emitted).\n\n### Encapsulation\nSometimes you wish to encapsulate a part of an ETL process and also use it elsewhere. It is possible\nto set an input pump and expose buffers from the group, so it will provide the same interface as a\nsimple pump (i.e. it has `.from()`, `.start()`, `.buffer()` methods and emits `end` event).\n\nMost likely, you want to extend `datapumps.Group` class (example is written in CoffeeScript):\n```coffee\n{ Group, mixin: { MysqlMixin } } = require 'datapumps'\n\nclass Notifier extends Group\n  constructor: ->\n    super()\n    @addPump 'emailLookup'\n      .mixin MysqlMixin connection\n      .process (data) ->\n        @query('SELECT email FROM user where username = ?', [ data.username ])\n          .then (result) =>\n            data.emailAddress = result.email\n            @buffer().writeAsync data\n    @addPump 'sendMail'\n      .from @pump 'emailLookup'\n      .process (data) ->\n        ... # send email to data.emailAddress\n        @buffer().writeAsync\n          recipient:\n            name: data.name\n            email: data.emailAddress\n\n    @setInputPump 'emailLookup'\n    @expose 'output', 'sendMail/output'\n```\nThe `Notifier` will behave like pump, but in the inside, it does an email address lookup using\nmysql, and sends mail to those addresses. The output buffer of `sendMail` pump is filled with\nrecipient data.\n\nUse the created class like this:\n```coffee\netlProcess = datapumps.group()\netlProcess\n  .addPump 'notifier', new Notifier\n    .from <node stream or datapumps buffer>\n\netlProcess\n  .addPump 'logger'\n    .from etlProcess.pump('notifier').buffer()\n    .process (data) ->\n      console.log \"Email sent to #{data.name} (#{data.email})\"\n```\nPlease note that you cannot use `.process` method on a group.\n\n## Error handling\nErrors may occur while data is transferred between systems. Most of the time, you don't want to stop\non the first error but complete the transfer and re-run after fixing problems. Therefore\nthe pump group has an error buffer (`.errorBuffer()`) which can hold ten error messages by default.\nWhen the error buffer fills up, `error` event is triggered and `.whenFinised()` promise is rejected:\n```js\ngroup\n  .start()\n  .whenFinished()\n    .then(function() {\n      if (!group.errorBuffer().isEmpty()) {\n        console.log(\"Transfer finished, but with errors.\");\n        // errors list will be at group.errorBuffer().getContent()\n      }\n    })\n    .catch(function() {\n      console.log(\"Pump group failed with errors\");\n      // errors list will be at group.errorBuffer().getContent()\n    });\n```\n\nYou can use the `.logErrorsToConsole()` helper method will configure the pump or group to print\nerrors when processing finished:\n```js\ngroup\n  .logErrorsToConsole()\n  .start();\n```\n\nYou can use the `.logErrorsToLogger()` helper method will configure the pump or group to print\nerrors to a logger when processing finished:\n```js\ngroup\n  .logErrorsToLogger(logger)\n  .start();\n```\n\nThis is useful for running the ETL on a server. The logger can be any logging method that contains\nan `.error()` method such as Winston, Log4js, etc.\n\n### Debugging\nThe following example shows a fingers-crossed type logging, i.e. debug logging is turned on\nafter the first error occured:\n\n```coffee\n{ group } = require('datapumps')\n\n(d = group())\n  .addPump 'test'\n    .from d.createBuffer\n      sealed: true,\n      content: [ 'first', 'second', 'third', 'fourth' ]\n    .process (data) ->\n      throw new Error 'Start debugging', data if data == 'second'\n      @copy data\n\nd.errorBuffer().on 'write', (data) ->\n  console.log data\n  d.buffer('test/output').on 'write', (data) ->\n    console.log \"#{data} was written to test/output buffer\"\n\nd.start()\n```\n\nThe output:\n```\n{ message: [Error: Start debugging], pump: 'test' }\nthird was written to test/output buffer\nfourth was written to test/output buffer\n```\n\n## Mixins\nThe core components of datapumps is only responsible for passing data in a flow-controlled manner.\nThe features required for import, export or transfer is provided by mixins:\n * [BatchMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/BatchMixin.html) - Processes input in batches. Useful with MysqlMixin or other database writing mixins (batch insert can be much faster than inserting one by one).\n * [MergeMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/MergeMixin.html) - Enables pump to read from multiple input buffers.\n * [ObjectTransformMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/ObjectTransformMixin.html) - Common object transformation and validation methods\n * [CsvWriterMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/CsvWriterMixin.html) - Writes csv files using fast-csv package\n * [ExcelWriterMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/ExcelWriterMixin.html) - Writes excel xlsx workbooks\n * [ExcelReaderMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/ExcelReaderMixin.html) - Reads excel xlsx workbooks\n * [MysqlMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/MysqlMixin.html) - Queries and writes mysql databases\n * [PostgresqlMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/PostgresqlMixin.html) - Queries and writes postgresql databases\n * [MongodbMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/MongodbMixin.html) - Queries and writes mongodb\n * [RestMixin](http://agmen-hu.github.io/node-datapumps/docs/mixin/RestMixin.html) - Interact with REST services\n\nWhen you implement new mixins, please fork datapumps and make a pull request.\n",
  "readmeFilename": "README.md",
  "_id": "datapumps@0.5.1",
  "_from": "datapumps@"
}
